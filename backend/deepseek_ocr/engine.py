"""
DeepSeek OCR è§£æå¼•æ“
å•ä¾‹æ¨¡å¼ï¼Œæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
"""
import os
import torch
from pathlib import Path
from typing import Optional, Dict, Any
from threading import Lock
from loguru import logger


class DeepSeekOCREngine:
    """
    DeepSeek OCR è§£æå¼•æ“
    
    ç‰¹æ€§ï¼š
    - å•ä¾‹æ¨¡å¼ï¼ˆæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹ï¼‰
    - ä¼˜å…ˆä» ModelScope ä¸‹è½½
    - çº¿ç¨‹å®‰å…¨
    """
    
    _instance: Optional['DeepSeekOCREngine'] = None
    _lock = Lock()
    _model = None
    _tokenizer = None
    _initialized = False
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, cache_dir: Optional[str] = None, auto_download: bool = True):
        """
        åˆå§‹åŒ–å¼•æ“ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
        
        Args:
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®ç›®å½•ä¸‹çš„ models/deepseek_ocr
            auto_download: æ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰
        """
        if self._initialized:
            return
        
        with self._lock:
            if self._initialized:
                return
            
            self.model_name = 'deepseek-ai/DeepSeek-OCR'
            
            # é»˜è®¤ç¼“å­˜ç›®å½•ï¼šé¡¹ç›®æ ¹ç›®å½•/models/deepseek_ocr
            if cache_dir is None:
                # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆbackend çš„çˆ¶ç›®å½•ï¼‰
                project_root = Path(__file__).parent.parent.parent
                self.cache_dir = str(project_root / 'models' / 'deepseek_ocr')
            else:
                self.cache_dir = cache_dir
            
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
            
            self.device = self._auto_select_device()
            self._initialized = True
            
            logger.info(f"ğŸ”§ DeepSeek OCR Engine initialized")
            logger.info(f"   Model: {self.model_name}")
            logger.info(f"   Device: {self.device}")
            logger.info(f"   Cache: {self.cache_dir}")
            
            # æ€»æ˜¯æ£€æŸ¥æœ¬åœ°æ¨¡å‹
            # auto_download=False æ—¶åªæ£€æŸ¥ä¸ä¸‹è½½
            self._check_local_model()
            
            # å¦‚æœéœ€è¦ä¸‹è½½ä¸”æœ¬åœ°ä¸å­˜åœ¨ï¼Œæ‰ä¸‹è½½
            if auto_download and self.model_name == 'deepseek-ai/DeepSeek-OCR':
                self._ensure_model_downloaded()
    
    def _auto_select_device(self) -> str:
        """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
        if torch.cuda.is_available():
            return 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'
        else:
            logger.error("=" * 80)
            logger.error("âŒ CUDA ä¸å¯ç”¨! DeepSeek OCR æ¨¡å‹éœ€è¦ GPU æ”¯æŒ")
            logger.error("")
            logger.error("ğŸ“‹ å¯èƒ½çš„åŸå› :")
            logger.error("   1. æ‚¨çš„ç”µè„‘æ²¡æœ‰ NVIDIA æ˜¾å¡")
            logger.error("   2. æ²¡æœ‰å®‰è£… CUDA é©±åŠ¨")
            logger.error("   3. å®‰è£…çš„æ˜¯ CPU ç‰ˆæœ¬çš„ PyTorch")
            logger.error("")
            logger.error("ğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            logger.error("   å¦‚æœæ‚¨æœ‰ NVIDIA æ˜¾å¡,è¯·å®‰è£… GPU ç‰ˆæœ¬çš„ PyTorch:")
            logger.error("   ")
            logger.error("   # å¸è½½å½“å‰ç‰ˆæœ¬")
            logger.error("   pip uninstall torch torchvision torchaudio")
            logger.error("   ")
            logger.error("   # å®‰è£… CUDA 11.8 ç‰ˆæœ¬")
            logger.error("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
            logger.error("   ")
            logger.error("   # æˆ–è€…å®‰è£… CUDA 12.1 ç‰ˆæœ¬")
            logger.error("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
            logger.error("")
            logger.error("=" * 80)
            
            raise RuntimeError(
                "DeepSeek OCR éœ€è¦ GPU æ”¯æŒã€‚è¯·å®‰è£… GPU ç‰ˆæœ¬çš„ PyTorch æˆ–ä½¿ç”¨å¸¦æœ‰ NVIDIA æ˜¾å¡çš„ç”µè„‘ã€‚\n"
                "è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ä¸Šæ–¹é”™è¯¯ä¿¡æ¯ã€‚"
            )
    
    def _check_local_model(self):
        """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆä¸ä¸‹è½½ï¼‰"""
        local_model_path = Path(self.cache_dir) / 'deepseek-ai' / 'DeepSeek-OCR'
        
        if local_model_path.exists():
            # æ£€æŸ¥å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶
            required_files = ['config.json', 'tokenizer.json', 'modeling_deepseekocr.py', 'model-00001-of-000001.safetensors']
            missing_files = [f for f in required_files if not (local_model_path / f).exists()]
            
            if not missing_files:
                # æœ¬åœ°æ¨¡å‹å®Œæ•´ï¼Œæ›´æ–° model_name ä¸ºæœ¬åœ°è·¯å¾„
                self.model_name = str(local_model_path)
                logger.info(f"âœ… Found complete local model at: {local_model_path}")
            else:
                logger.debug(f"   Local model incomplete, missing: {missing_files}")
        else:
            logger.debug(f"   Local model directory not found: {local_model_path}")
    
    def _ensure_model_downloaded(self):
        """ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½ï¼ˆä¸åŠ è½½åˆ°å†…å­˜ï¼‰"""
        try:
            logger.info("ğŸ” Checking if model is downloaded...")
            
            # é¦–å…ˆæ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²ç»æœ‰å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶
            local_model_path = Path(self.cache_dir) / 'deepseek-ai' / 'DeepSeek-OCR'
            logger.debug(f"   Checking local path: {local_model_path}")
            
            # æ£€æŸ¥å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = ['config.json', 'tokenizer.json', 'modeling_deepseekocr.py', 'model-00001-of-000001.safetensors']
            
            if local_model_path.exists():
                missing_files = [f for f in required_files if not (local_model_path / f).exists()]
                
                if not missing_files:
                    # æœ¬åœ°æ¨¡å‹å®Œæ•´ï¼Œç›´æ¥ä½¿ç”¨
                    self.model_name = str(local_model_path)
                    logger.info(f"âœ… Local model is complete at: {local_model_path}")
                    logger.info("   Skipping download, will use local files")
                    return
                else:
                    logger.warning(f"âš ï¸  Local model incomplete, missing files: {missing_files}")
            else:
                logger.info(f"   Local model directory not found")
            
            # åªæœ‰åœ¨æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´æ—¶æ‰ä¸‹è½½
            logger.info("ğŸ“¥ Local model not found or incomplete, starting download...")
            
            # é…ç½®ä¸‹è½½æº
            os.environ.setdefault('HF_ENDPOINT', 'https://hf-mirror.com')
            
            try:
                # ä¼˜å…ˆä½¿ç”¨ ModelScope
                from modelscope import snapshot_download
                
                logger.info(f"ğŸ“¦ Using ModelScope to download model")
                
                # ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰
                model_dir = snapshot_download(
                    'deepseek-ai/DeepSeek-OCR',
                    cache_dir=self.cache_dir
                )
                
                self.model_name = model_dir  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
                logger.info(f"âœ… Model downloaded at: {model_dir}")
                
            except ImportError:
                # ModelScope ä¸å¯ç”¨ï¼Œä½¿ç”¨ HuggingFace
                logger.info("ğŸ“¦ Using HuggingFace Hub to download")
                
                # åªä¸‹è½½æ¨¡å‹é…ç½®ï¼Œä¸åŠ è½½æ¨¡å‹
                # è¿™ä¼šè§¦å‘æ¨¡å‹æ–‡ä»¶ä¸‹è½½ä½†ä¸å ç”¨ GPU/å†…å­˜
                try:
                    from transformers import AutoConfig
                    AutoConfig.from_pretrained(
                        self.model_name,
                        trust_remote_code=True,
                        cache_dir=self.cache_dir
                    )
                    logger.info(f"âœ… Model ready (will load on first use)")
                except Exception as e:
                    logger.warning(f"âš ï¸  Could not verify model: {e}")
                    logger.info("   Model will be downloaded on first use")
                    
        except Exception as e:
            logger.warning(f"âš ï¸  Model check/download failed: {e}")
            logger.info("   Model will be downloaded on first use")
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self._model is not None and self._tokenizer is not None:
            return self._model, self._tokenizer
        
        with self._lock:
            if self._model is not None and self._tokenizer is not None:
                return self._model, self._tokenizer
            
            logger.info("=" * 60)
            logger.info("ğŸ“¥ Loading DeepSeek OCR Model into memory...")
            logger.info("=" * 60)
            
            try:
                from transformers import AutoTokenizer, AutoModel
                
                # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆè·¯å¾„è€Œéæ¨¡å‹IDï¼‰
                is_local = Path(self.model_name).exists()
                
                if is_local:
                    logger.info(f"ğŸ“ Loading from local path: {self.model_name}")
                else:
                    logger.info(f"ğŸŒ Loading from HuggingFace: {self.model_name}")
                
                # åŠ è½½ tokenizer
                logger.info(f"ğŸ“ Loading tokenizer...")
                self._tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    trust_remote_code=True,
                    local_files_only=is_local,
                    cache_dir=None if is_local else self.cache_dir
                )
                
                # åŠ è½½æ¨¡å‹
                logger.info(f"ğŸ¤– Loading model...")
                
                try:
                    # å°è¯•ä½¿ç”¨ flash attention 2ï¼ˆéœ€è¦ flash-attn åŒ…ï¼‰
                    self._model = AutoModel.from_pretrained(
                        self.model_name,
                        _attn_implementation='flash_attention_2',
                        trust_remote_code=True,
                        use_safetensors=True,
                        local_files_only=is_local,
                        cache_dir=None if is_local else self.cache_dir
                    )
                    logger.info("âœ… Using flash_attention_2")
                except Exception as e:
                    # Flash attention ä¸å¯ç”¨æ—¶å›é€€åˆ°é»˜è®¤å®ç°
                    logger.warning(f"âš ï¸  Flash attention not available, using default attention")
                    logger.debug(f"   Reason: {e}")
                    
                    self._model = AutoModel.from_pretrained(
                        self.model_name,
                        trust_remote_code=True,
                        use_safetensors=True,
                        local_files_only=is_local,
                        cache_dir=None if is_local else self.cache_dir
                    )
                    logger.info("âœ… Using default attention implementation")
                
                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ç§»åˆ°è®¾å¤‡
                self._model = self._model.eval()
                
                logger.info(f"ğŸ“¤ Moving model to device: {self.device}")
                
                if self.device == 'cuda':
                    # è®°å½•ç§»åŠ¨å‰çš„ GPU çŠ¶æ€
                    gpu_memory_before = torch.cuda.memory_allocated(0) / 1024**3
                    logger.info(f"   GPU æ˜¾å­˜ (ç§»åŠ¨å‰): {gpu_memory_before:.2f}GB")
                    
                    self._model = self._model.cuda().to(torch.bfloat16)
                    
                    # è®°å½•ç§»åŠ¨åçš„ GPU çŠ¶æ€
                    gpu_memory_after = torch.cuda.memory_allocated(0) / 1024**3
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    logger.info(f"   GPU æ˜¾å­˜ (ç§»åŠ¨å): {gpu_memory_after:.2f}GB / {gpu_memory_total:.2f}GB")
                    logger.info(f"   æ¨¡å‹å ç”¨æ˜¾å­˜: {gpu_memory_after - gpu_memory_before:.2f}GB")
                    
                elif self.device == 'mps':
                    self._model = self._model.to('mps')
                
                logger.info("=" * 60)
                logger.info("âœ… DeepSeek OCR Model loaded successfully!")
                logger.info(f"   Device: {self.device}")
                if self.device == 'cuda':
                    logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")
                    logger.info(f"   æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
                    logger.info(f"   å·²ç”¨æ˜¾å­˜: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
                logger.info("=" * 60)
                
                return self._model, self._tokenizer
                
            except Exception as e:
                logger.error("=" * 80)
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥:")
                logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
                logger.error("")
                logger.error("ğŸ’¡ æ’æŸ¥å»ºè®®:")
                logger.error("   1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
                logger.error("   2. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
                logger.error("   3. æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤ŸåŠ è½½æ¨¡å‹")
                if self.device == 'cuda':
                    try:
                        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                        logger.error(f"   4. å½“å‰ GPU æ˜¾å­˜: {gpu_memory_total:.2f}GB")
                    except:
                        pass
                logger.error("=" * 80)
                
                import traceback
                logger.debug("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
                logger.debug(traceback.format_exc())
                
                raise
    
    def _convert_pdf_to_images(self, pdf_path: Path, output_dir: Path) -> list:
        """
        å°† PDF æ‰€æœ‰é¡µè½¬æ¢ä¸ºå›¾ç‰‡
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è½¬æ¢åçš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        # ä½¿ç”¨å…¬å…±å·¥å…·å‡½æ•°è½¬æ¢æ‰€æœ‰é¡µ
        from utils.pdf_utils import convert_pdf_to_images
        return convert_pdf_to_images(pdf_path, output_dir)
    
    def cleanup(self):
        """
        æ¸…ç†æ¨ç†äº§ç”Ÿçš„æ˜¾å­˜ï¼ˆä¸å¸è½½æ¨¡å‹ï¼‰
        
        æ³¨æ„ï¼š
        - åªæ¸…ç†æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸­é—´å¼ é‡
        - ä¸ä¼šå¸è½½å·²åŠ è½½çš„æ¨¡å‹ï¼ˆæ¨¡å‹ä¿æŒåœ¨æ˜¾å­˜ä¸­ï¼Œä¸‹æ¬¡æ¨ç†æ›´å¿«ï¼‰
        - é€‚åˆåœ¨æ¯æ¬¡æ¨ç†å®Œæˆåè°ƒç”¨
        """
        try:
            import gc
            
            # æ¸…ç† PyTorch æ˜¾å­˜ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
                logger.debug("ğŸ§¹ DeepSeek OCR: CUDA cache cleared")
            
            # æ¸…ç† Python å¯¹è±¡
            gc.collect()
            
            logger.debug("ğŸ§¹ DeepSeek OCR: Memory cleanup completed")
        except Exception as e:
            logger.debug(f"Memory cleanup warning: {e}")
    
    def parse(
        self,
        file_path: str,
        output_path: str,
        resolution: str = 'base',
        prompt_type: str = 'document',
        **kwargs
    ) -> Dict[str, Any]:
        """
        è§£ææ–‡æ¡£æˆ–å›¾ç‰‡
        
        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºç›®å½•
            resolution: åˆ†è¾¨ç‡ (tiny/small/base/large/dynamic)
            prompt_type: æç¤ºè¯ç±»å‹ (document/image/free/figure)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è§£æç»“æœ
        """
        file_path = Path(file_path)
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ¤– DeepSeek OCR parsing: {file_path.name}")
        logger.info(f"   Resolution: {resolution}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼Œå¦‚æœæ˜¯ PDF éœ€è¦å…ˆè½¬æ¢ä¸ºå›¾ç‰‡
        image_paths = []
        if file_path.suffix.lower() == '.pdf':
            logger.info("ğŸ“„ PDF detected, converting all pages to images...")
            image_paths = self._convert_pdf_to_images(file_path, output_path)
            logger.info(f"âœ… PDF converted: {len(image_paths)} pages")
        else:
            # å•å¼ å›¾ç‰‡
            image_paths = [file_path]
        
        # åŠ è½½æ¨¡å‹
        model, tokenizer = self._load_model()
        
        # æç¤ºè¯æ¨¡æ¿
        prompts = {
            'document': '<image>\n<|grounding|>Convert the document to markdown.',
            'image': '<image>\n<|grounding|>OCR this image.',
            'free': '<image>\nFree OCR.',
            'figure': '<image>\nParse the figure.',
        }
        prompt = prompts.get(prompt_type, prompts['document'])
        
        # åˆ†è¾¨ç‡é…ç½®
        resolutions = {
            'tiny': {'base_size': 512, 'image_size': 512},
            'small': {'base_size': 640, 'image_size': 640},
            'base': {'base_size': 1024, 'image_size': 1024},
            'large': {'base_size': 1280, 'image_size': 1280},
            'dynamic': {'base_size': 1024, 'image_size': 640},
        }
        res_config = resolutions.get(resolution, resolutions['base'])
        
        # æ‰§è¡Œæ¨ç†ï¼ˆå¤„ç†æ‰€æœ‰é¡µï¼‰
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ¨ç†...")
            logger.info(f"   åˆ†è¾¨ç‡é…ç½®: base_size={res_config['base_size']}, image_size={res_config['image_size']}")
            logger.info(f"   å…± {len(image_paths)} ä¸ªå›¾åƒ")
            logger.info(f"   æç¤ºè¯ç±»å‹: {prompt_type}")
            
            # è®°å½• GPU çŠ¶æ€
            if self.device == 'cuda':
                gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"   GPU æ˜¾å­˜: å·²åˆ†é… {gpu_memory_allocated:.2f}GB / å·²ä¿ç•™ {gpu_memory_reserved:.2f}GB / æ€»è®¡ {gpu_memory_total:.2f}GB")
            
            all_markdown_content = []
            
            # å¤„ç†æ¯ä¸ªå›¾åƒï¼ˆæ¯é¡µï¼‰
            for idx, img_path in enumerate(image_paths, 1):
                logger.info(f"ğŸ“ å¤„ç†ç¬¬ {idx}/{len(image_paths)} é¡µ: {img_path.name}")
                
                # ä¸ºæ¯é¡µåˆ›å»ºå­ç›®å½•
                page_output_dir = output_path / f"page_{idx}"
                page_output_dir.mkdir(parents=True, exist_ok=True)
                
                result = model.infer(
                    tokenizer,
                    prompt=prompt,
                    image_file=str(img_path),
                    output_path=str(page_output_dir),
                    base_size=res_config['base_size'],
                    image_size=res_config['image_size'],
                    crop_mode=True,
                    save_results=True,
                    test_compress=True
                )
                
                # è¯»å–è¿™ä¸€é¡µçš„ MMD æ–‡ä»¶
                page_mmd_file = page_output_dir / 'result.mmd'
                if page_mmd_file.exists():
                    try:
                        with open(page_mmd_file, 'r', encoding='utf-8') as f:
                            page_content = f.read()
                        
                        # æ·»åŠ é¡µæ ‡è®°
                        all_markdown_content.append(f"\n\n## ç¬¬ {idx} é¡µ\n\n")
                        all_markdown_content.append(page_content)
                        
                        logger.info(f"   âœ… ç¬¬ {idx} é¡µå¤„ç†å®Œæˆ")
                    except Exception as e:
                        logger.warning(f"   âš ï¸  è¯»å–ç¬¬ {idx} é¡µå¤±è´¥: {e}")
                else:
                    logger.warning(f"   âš ï¸  ç¬¬ {idx} é¡µæœªç”Ÿæˆ MMD æ–‡ä»¶")
            
            logger.info(f"âœ… DeepSeek OCR completed - å…±å¤„ç† {len(image_paths)} é¡µ")
            
            # åˆå¹¶æ‰€æœ‰é¡µçš„å†…å®¹
            markdown_content = ''.join(all_markdown_content)
            
            # ä¿å­˜åˆå¹¶åçš„ç»“æœåˆ°ä¸»ç›®å½•
            merged_mmd_file = output_path / 'result.mmd'
            if markdown_content:
                try:
                    merged_mmd_file.write_text(markdown_content, encoding='utf-8')
                    logger.info(f"ğŸ“„ å·²åˆå¹¶æ‰€æœ‰é¡µ: {len(markdown_content)} å­—ç¬¦")
                except Exception as e:
                    logger.warning(f"âš ï¸  ä¿å­˜åˆå¹¶æ–‡ä»¶å¤±è´¥: {e}")
            else:
                logger.warning(f"âš ï¸  æ²¡æœ‰å†…å®¹å¯åˆå¹¶")
            
            mmd_file = merged_mmd_file
            
            return {
                'success': True,
                'output_path': str(output_path),
                'markdown': markdown_content,  # è¿”å› Markdown å†…å®¹
                'mmd_file': str(mmd_file) if mmd_file.exists() else None,
                'result': result  # ä¿ç•™åŸå§‹ç»“æœ
            }
        
        except RuntimeError as e:
            error_msg = str(e)
            logger.error("=" * 80)
            logger.error(f"âŒ RuntimeError å¼‚å¸¸è¯¦æƒ…:")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
            logger.error("")
            
            # CUDA ç›¸å…³é”™è¯¯åˆ†æ
            if 'CUDA' in error_msg or 'cuda' in error_msg or 'CUBLAS' in error_msg:
                logger.error("ğŸ” CUDA é”™è¯¯åˆ†æ:")
                
                # æ˜¾å­˜ç›¸å…³
                if 'out of memory' in error_msg.lower():
                    logger.error("   ç±»å‹: GPU æ˜¾å­˜ä¸è¶³ (OOM)")
                    if self.device == 'cuda':
                        try:
                            gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                            gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                            logger.error(f"   å½“å‰æ˜¾å­˜: å·²åˆ†é… {gpu_memory_allocated:.2f}GB / å·²ä¿ç•™ {gpu_memory_reserved:.2f}GB / æ€»è®¡ {gpu_memory_total:.2f}GB")
                        except:
                            pass
                    logger.error("")
                    logger.error("ğŸ’¡ å»ºè®®:")
                    logger.error("   1. ä½¿ç”¨æ›´å°çš„åˆ†è¾¨ç‡: 'small' æˆ– 'tiny'")
                    logger.error("   2. é‡å¯ worker è¿›ç¨‹é‡Šæ”¾æ˜¾å­˜")
                    logger.error("   3. å…³é—­å…¶ä»–å ç”¨ GPU çš„ç¨‹åº")
                
                # CUBLAS é”™è¯¯
                elif 'CUBLAS' in error_msg or 'cublas' in error_msg:
                    logger.error("   ç±»å‹: CUBLAS çŸ©é˜µè¿ç®—é”™è¯¯")
                    logger.error("")
                    logger.error("ğŸ” å¯èƒ½åŸå› :")
                    logger.error("   1. æ˜¾å­˜ç¢ç‰‡åŒ–æˆ–åˆ†é…å¤±è´¥")
                    logger.error("   2. è¾“å…¥å¼ é‡å°ºå¯¸å¯¼è‡´ç´¢å¼•è¶Šç•Œ")
                    logger.error("   3. CUDA é©±åŠ¨ä¸ PyTorch ç‰ˆæœ¬ä¸åŒ¹é…")
                    logger.error("   4. GPU çŠ¶æ€å¼‚å¸¸")
                    logger.error("")
                    logger.error("ğŸ’¡ å»ºè®®:")
                    logger.error("   1. é‡å¯ worker è¿›ç¨‹ (æœ€é‡è¦!)")
                    logger.error("   2. ä½¿ç”¨æ›´å°çš„åˆ†è¾¨ç‡")
                    logger.error("   3. æ£€æŸ¥ CUDA é©±åŠ¨ç‰ˆæœ¬: nvidia-smi")
                    if self.device == 'cuda':
                        try:
                            logger.error(f"   4. PyTorch CUDA ç‰ˆæœ¬: {torch.version.cuda}")
                            logger.error(f"   5. GPU å‹å·: {torch.cuda.get_device_name(0)}")
                        except:
                            pass
                
                # æ–­è¨€å¤±è´¥
                elif 'assertion' in error_msg.lower() or 'assert' in error_msg.lower():
                    logger.error("   ç±»å‹: CUDA å†…æ ¸æ–­è¨€å¤±è´¥")
                    logger.error("")
                    logger.error("ğŸ” å¯èƒ½åŸå› :")
                    logger.error("   1. å¼ é‡ç´¢å¼•è¶Šç•Œ (ç´¢å¼•è¶…å‡ºæ•°æ®èŒƒå›´)")
                    logger.error("   2. è¾“å…¥æ•°æ®å½¢çŠ¶ä¸æ¨¡å‹æœŸæœ›ä¸åŒ¹é…")
                    logger.error("   3. æ˜¾å­˜æŸåæˆ–å¼‚å¸¸")
                    logger.error("")
                    logger.error("ğŸ’¡ å»ºè®®:")
                    logger.error("   1. é‡å¯ worker è¿›ç¨‹")
                    logger.error("   2. æ£€æŸ¥è¾“å…¥å›¾åƒæ˜¯å¦æŸå")
                    logger.error("   3. å°è¯•ä½¿ç”¨ä¸åŒçš„åˆ†è¾¨ç‡")
                    logger.error(f"   4. å½“å‰åˆ†è¾¨ç‡: {resolution} (base_size={res_config['base_size']})")
                
                else:
                    logger.error("   ç±»å‹: å…¶ä»– CUDA é”™è¯¯")
                    logger.error("")
                    logger.error("ğŸ’¡ å»ºè®®:")
                    logger.error("   1. é‡å¯ worker è¿›ç¨‹")
                    logger.error("   2. æ£€æŸ¥ nvidia-smi è¾“å‡º")
                    logger.error("   3. å°è¯•é™ä½åˆ†è¾¨ç‡")
                
                # GPU çŠ¶æ€ä¿¡æ¯
                if self.device == 'cuda':
                    try:
                        logger.error("")
                        logger.error("ğŸ“Š å½“å‰ GPU çŠ¶æ€:")
                        gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                        gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                        gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                        logger.error(f"   æ˜¾å­˜: {gpu_memory_allocated:.2f}GB / {gpu_memory_total:.2f}GB (å·²åˆ†é…)")
                        logger.error(f"   æ˜¾å­˜: {gpu_memory_reserved:.2f}GB / {gpu_memory_total:.2f}GB (å·²ä¿ç•™)")
                        logger.error(f"   è®¾å¤‡: {torch.cuda.get_device_name(0)}")
                    except Exception as gpu_err:
                        logger.error(f"   æ— æ³•è·å– GPU çŠ¶æ€: {gpu_err}")
            
            else:
                logger.error("ğŸ” ä¸€èˆ¬ RuntimeError:")
                logger.error(f"   è¯¦ç»†ä¿¡æ¯: {error_msg}")
            
            logger.error("=" * 80)
            
            # ä¿ç•™å®Œæ•´çš„å †æ ˆä¿¡æ¯
            import traceback
            logger.debug("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            logger.debug(traceback.format_exc())
            
            raise
            
        except ZeroDivisionError as e:
            # ä¸“é—¨å¤„ç†é™¤é›¶é”™è¯¯ï¼ˆé€šå¸¸æ˜¯åˆ†è¾¨ç‡å¤ªå°å¯¼è‡´ï¼‰
            logger.error("=" * 80)
            logger.error(f"âŒ åˆ†è¾¨ç‡é…ç½®é”™è¯¯:")
            logger.error(f"   é”™è¯¯ç±»å‹: ZeroDivisionError")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {e}")
            logger.error("")
            logger.error("ğŸ” åŸå› åˆ†æ:")
            logger.error(f"   å½“å‰åˆ†è¾¨ç‡ '{resolution}' å¯¹äºæ­¤å›¾åƒæ¥è¯´å¤ªå°")
            logger.error("   æ¨¡å‹å†…éƒ¨è®¡ç®—çš„ valid_img_tokens = 0")
            logger.error("   å¯¼è‡´åœ¨è®¡ç®—å‹ç¼©æ¯”æ—¶é™¤ä»¥é›¶")
            logger.error("")
            logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            logger.error("   1. ä½¿ç”¨æ›´å¤§çš„åˆ†è¾¨ç‡:")
            logger.error("      - å¦‚æœå½“å‰æ˜¯ 'tiny',  æ”¹ç”¨ 'small'")
            logger.error("      - å¦‚æœå½“å‰æ˜¯ 'small', æ”¹ç”¨ 'base'")
            logger.error("   2. æ£€æŸ¥è¾“å…¥å›¾åƒæ˜¯å¦æ­£å¸¸")
            logger.error(f"   3. å½“å‰åˆ†è¾¨ç‡é…ç½®: {resolution} (base_size={res_config['base_size']})")
            logger.error("")
            logger.error("ğŸ“Š å»ºè®®çš„åˆ†è¾¨ç‡é€‰æ‹©:")
            logger.error("   - ç®€å•æ–‡æ¡£/å°å›¾: small (640x640)")
            logger.error("   - æ ‡å‡†æ–‡æ¡£:      base  (1024x1024)")
            logger.error("   - å¤æ‚æ–‡æ¡£:      large (1280x1280)")
            logger.error("=" * 80)
            
            # ä¿ç•™å®Œæ•´çš„å †æ ˆä¿¡æ¯
            import traceback
            logger.debug("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            logger.debug(traceback.format_exc())
            
            raise RuntimeError(
                f"åˆ†è¾¨ç‡ '{resolution}' å¯¹äºæ­¤å›¾åƒæ¥è¯´å¤ªå°ã€‚"
                f"è¯·ä½¿ç”¨æ›´å¤§çš„åˆ†è¾¨ç‡ (å»ºè®®: small æˆ– base)ã€‚"
            )
            
        except Exception as e:
            error_msg = str(e)
            logger.error("=" * 80)
            logger.error(f"âŒ æœªé¢„æœŸçš„å¼‚å¸¸:")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
            logger.error("")
            logger.error("ğŸ’¡ å»ºè®®:")
            logger.error("   1. æŸ¥çœ‹å®Œæ•´å †æ ˆè·Ÿè¸ªå®šä½é—®é¢˜")
            logger.error("   2. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦æ­£å¸¸")
            logger.error("   3. éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§")
            logger.error("=" * 80)
            
            # ä¿ç•™å®Œæ•´çš„å †æ ˆä¿¡æ¯
            import traceback
            logger.debug("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            logger.debug(traceback.format_exc())
            
            raise
        
        finally:
            # æ¸…ç†æ˜¾å­˜ï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½æ‰§è¡Œï¼‰
            self.cleanup()


# å…¨å±€å•ä¾‹
_engine = None

def get_engine() -> DeepSeekOCREngine:
    """è·å–å…¨å±€å¼•æ“å®ä¾‹"""
    global _engine
    if _engine is None:
        _engine = DeepSeekOCREngine()
    return _engine

